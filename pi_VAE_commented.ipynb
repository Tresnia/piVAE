{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#print(torch.cuda.is_available())\n",
    "#print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise the Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimension of beta_i - there are as many beta-i's as there are functions but this is the dimension of each beta_i\n",
    "beta_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our inputs are scalars of dimension 1\n",
    "input_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check what this correspons to - number of phi_rbfs'\n",
    "num_phi_rbf = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the variance of the radial basis function kernels\n",
    "phi_rbf_sigma = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of neuron on each hidden layer of the function Phi\n",
    "phi_hidden_layer_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does z_dim represent?\n",
    "z_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the numbers of betas to learn\n",
    "num_training_funcs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points each function is evaluated at\n",
    "#this is the K in the paper\n",
    "num_eval_points = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observation standard deviation\n",
    "obs_sigma = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.linear applies a linear transformation to the incoming data/inputs. \n",
    "#this defines the dimensions of the hidden layers inside the encoder?\n",
    "encoder_h_dim_1 = 512\n",
    "encoder_h_dim_2 = 512\n",
    "encoder_h_dim_3 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_h_dim_1 = 128\n",
    "decoder_h_dim_2 = 128\n",
    "decoder_h_dim_3 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interval of definition for the x's (function evaluations)\n",
    "function_xlims = [-5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that produces a cubic dataset: x^3+noise\n",
    "\n",
    "def generate_cubic_dataset():\n",
    "    #create 10 points uniformly sampled between -4 and -2\n",
    "    x_points = np.random.uniform(low=-4, high=-2, size=(10,))\n",
    "    #creaet 10 new points uniformly sampled between 2 and 4 and append them to the prvious 10 points\n",
    "    x_points = np.append(x_points, np.random.uniform(low=2, high=4, size=(10,)))\n",
    "    #compute y=x^3+Noise(normal(0,1)^3)\n",
    "    y_points = x_points**3 + np.random.normal(size=(20,)) * 3\n",
    "    #we return the 20 x's and the 20 y's\n",
    "    return (x_points, y_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From krasserm github io\n",
    "\n",
    "#isotropic squared exp kernel\n",
    "#np.reshape gives a new shape to an array without changing its data. \n",
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Isotropic squared exponential kernel.\n",
    "        \n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        (m x n) matrix.\n",
    "    \"\"\"\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the set of training functions - here prior is a GP\n",
    "#f_i ~ GP(0, cov(X,X))\n",
    "def generate_gp_1d_dataset():\n",
    "    # X = np.arange(function_xlims[0], function_xlims[1], 0.1).reshape(-1, 1)\n",
    "    output_X = []\n",
    "    output_samples = []\n",
    "    for n in range(num_training_funcs):\n",
    "        X = np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_eval_points,1))\n",
    "        mu = np.zeros(X.shape)\n",
    "        cov = kernel(X, X)\n",
    "        sample = np.random.multivariate_normal(mu.ravel(), cov, 1)\n",
    "        output_X.append(X)\n",
    "        output_samples.append(sample)\n",
    "    return np.array(output_X), np.array(output_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a set of training functions - here the prior is based on a sine wave\n",
    "\n",
    "#f_i=a. sqrt(|X|).sin(5X)+X^2\n",
    "\n",
    "def generate_quadratic_sine_dataset():\n",
    "    output_X = []\n",
    "    output_samples = []\n",
    "    for n in range(num_training_funcs):\n",
    "        X = np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_eval_points, 1))\n",
    "        a = np.random.uniform(0.0, 3.0)\n",
    "        y = a * np.sqrt(np.abs(X)) * np.sin(X*5) + X**2\n",
    "        output_X.append(X)\n",
    "        output_samples.append(y)\n",
    "    return np.array(output_X), np.array(output_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=A*exp(B*x), where A ~ U(1,2) and B ~ U(-2,2)\n",
    "\n",
    "def generate_exp_dataset():\n",
    "    output_X = []\n",
    "    output_samples = []\n",
    "    for n in range(num_training_funcs):\n",
    "        X = np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_eval_points, 1))\n",
    "        A = np.random.uniform(1, 2)\n",
    "        B = np.random.uniform(-2, 2)\n",
    "        y = A * np.exp(B * X)\n",
    "        output_X.append(X)\n",
    "        output_samples.append(y)\n",
    "    return np.array(output_X), np.array(output_samples)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y= a sin(x + phase) where a ~ U(0,1) and phase~U(0, pi)\n",
    "\n",
    "def generate_maml_sine_dataset():\n",
    "    output_X = []\n",
    "    output_samples = []\n",
    "    for n in range(num_training_funcs):\n",
    "        X = np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_eval_points, 1))\n",
    "        amplitude = np.random.uniform(0, 1.0)\n",
    "        phase = np.random.uniform(0, np.pi)\n",
    "        y = amplitude * np.sin(X + phase)\n",
    "        output_X.append(X)\n",
    "        output_samples.append(y)\n",
    "    return np.array(output_X), np.array(output_samples)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define a Model class which: \n",
    "    #initialises the arguments and parameters (what exactly needs initialisation)\n",
    "    #contains various function definitions: \n",
    "        #the high dimensional function Phis, \n",
    "        #the encoder, \n",
    "        #the decoder, \n",
    "        #the get_loss function\n",
    "        #the eval_at_z function: which gives predicted x values at the location points s, when the z value is given\n",
    "        #the draw_sample function: which draws samples from the pi vae (s should be (num_eval_points, dim))\n",
    "        # the get_unnormalized_log_posterior function which gets something proportional to p(z|x, s) where x and s are new test points \n",
    "            #s (batch x dim)\n",
    "            # x (batch)\n",
    "            # z (z_dim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of Model class without comments:\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.phi_rbf_centers = nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_phi_rbf, input_dim))))\n",
    "        self.phi_nn_1 = nn.Linear(num_phi_rbf, phi_hidden_layer_size)\n",
    "        self.phi_nn_2 = nn.Linear(phi_hidden_layer_size, beta_dim)\n",
    "\n",
    "        self.encoder_nn_1 = nn.Linear(beta_dim, encoder_h_dim_1)\n",
    "        self.encoder_nn_2 = nn.Linear(encoder_h_dim_1, encoder_h_dim_2)\n",
    "        self.encoder_nn_3 = nn.Linear(encoder_h_dim_2, encoder_h_dim_3)\n",
    "        self.encoder_nn_4 = nn.Linear(encoder_h_dim_3, z_dim * 2)\n",
    "\n",
    "        self.decoder_nn_1 = nn.Linear(z_dim, decoder_h_dim_1)\n",
    "        self.decoder_nn_2 = nn.Linear(decoder_h_dim_1, decoder_h_dim_2)\n",
    "        self.decoder_nn_3 = nn.Linear(decoder_h_dim_2, decoder_h_dim_3)\n",
    "        self.decoder_nn_4 = nn.Linear(decoder_h_dim_3, beta_dim)\n",
    "\n",
    "        # self.betas = nn.Parameter(torch.ones(num_training_funcs, beta_dim))\n",
    "        self.betas = nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(-1, 1, size=(num_training_funcs, beta_dim))\n",
    "        ))\n",
    "\n",
    "        self.normal_sampler = torch.distributions.normal.Normal(0.0, 1.0)\n",
    "\n",
    "    def Phi(self, input):\n",
    "        # Takes input (batch x dim_in) and gives Phi(input) (batch x dim_out)\n",
    "        #torch.unsqueeze returns a new tensor (here called input_expand)\n",
    "        #w ith a dimension of size 1 inserted at the specified position 1\n",
    "        #so here input_expand is transformed into a one-column vector\n",
    "        input_expand = torch.unsqueeze(input, 1)\n",
    "        #same here except that the phi_rbf_centers are transformed into a row vector\n",
    "        phi_expand = torch.unsqueeze(self.phi_rbf_centers, 0)\n",
    "        M1 = input_expand - phi_expand\n",
    "        M2 = torch.sum(M1 ** 2, 2)\n",
    "        M3 = torch.exp(-M2/phi_rbf_sigma)\n",
    "        M4 = F.sigmoid(self.phi_nn_1(M3))\n",
    "        M5 = self.phi_nn_2(M4)\n",
    "        return M5\n",
    "\n",
    "    def encoder(self, input):\n",
    "        # input (batch x beta_dim) output ((batch x z_dim), (batch x z_dim))\n",
    "        M1 = F.relu(self.encoder_nn_1(input))\n",
    "        M2 = F.relu(self.encoder_nn_2(M1))\n",
    "        M3 = F.relu(self.encoder_nn_3(M2))\n",
    "        M4 = self.encoder_nn_4(M3)\n",
    "        z_mean = M4[:, 0:z_dim]\n",
    "        z_std = torch.exp(M4[:, z_dim:]) # needs to be positive\n",
    "        return z_mean, z_std\n",
    "\n",
    "    def decoder(self, input):\n",
    "        # input (batch x z_dim) output (batch x beta_dim)\n",
    "        M1 = F.relu(self.decoder_nn_1(input))\n",
    "        M2 = F.relu(self.decoder_nn_2(M1))\n",
    "        M3 = F.relu(self.decoder_nn_3(M2))\n",
    "        M4 = self.decoder_nn_4(M3)\n",
    "        return M4\n",
    "\n",
    "    def get_loss(self, function_id, s, x, kl_factor, print_breakdown=False, \n",
    "        return_breakdown=False):\n",
    "        # function_id is just to know which beta to use\n",
    "        # s are the inputs (batch x dim)\n",
    "        # x are the observed outputs (batch)\n",
    "        batch_size = s.shape[0]\n",
    "\n",
    "        phi_s = self.Phi(s)\n",
    "        beta = self.betas[function_id, :]\n",
    "        x_enc = torch.matmul(phi_s, beta)\n",
    "\n",
    "        loss_term_1 = (x - x_enc)**2\n",
    "\n",
    "        z_mean, z_std = self.encoder(beta.unsqueeze(0))\n",
    "        # Do we draw one z_sample for all x values of this function or one z_sample for each of them?\n",
    "        # The pi-vae paper in Alg1 does one z-sample for all x-values of the function\n",
    "        z_sample = z_mean + z_std * self.normal_sampler.rsample((1, z_dim))\n",
    "        beta_hat = self.decoder(z_sample)\n",
    "        x_dec = torch.matmul(phi_s, beta_hat.squeeze()) # double check this is actually doing what we want it to\n",
    "        loss_term_2 = (x - x_dec)**2\n",
    "\n",
    "        # z_samples = z_mean + z_std * self.normal_sampler.rsample((batch_size, z_dim))\n",
    "        # z_samples = z_mean.repeat(batch_size, 1)\n",
    "        # beta_hats = self.decoder(z_samples)\n",
    "        # x_dec = torch.sum(beta_hats * phi_s, dim=1)\n",
    "        # loss_term_2 = (x - x_dec)**2\n",
    "        # beta_hat = self.decoder(z_mean)\n",
    "        # beta_hat = beta_hat.reshape(beta.shape)\n",
    "        # loss_term_2 = torch.mean((beta_hat - beta)**2)\n",
    "\n",
    "        \n",
    "\n",
    "        # You only get one value not batch_num values since there's only\n",
    "        # one beta for the whole batch since they're all from the same function\n",
    "        # But when you add all the losses together, it will get broadcasted\n",
    "        # so that it is repeated for each item in the batch so the mean will\n",
    "        # be ok\n",
    "        loss_term_3 = 0.5 * torch.sum(z_std**2 + z_mean**2 - 1 - torch.log(z_std**2),\n",
    "            dim=1)\n",
    "        loss_term_3 = kl_factor * (loss_term_3/z_dim)\n",
    "\n",
    "        if print_breakdown:\n",
    "            # print(\"z_mean, std\", z_mean, z_std)\n",
    "            # print(\"z_samples\", z_samples)\n",
    "            print(\"1\", torch.mean(loss_term_1))\n",
    "            print(\"2\", torch.mean(loss_term_2))\n",
    "            print(\"3\", loss_term_3)\n",
    "\n",
    "        if return_breakdown == False:\n",
    "            return torch.mean(loss_term_1 + loss_term_2) + loss_term_3\n",
    "        else:\n",
    "            return torch.mean(loss_term_1 + loss_term_2) + loss_term_3, \\\n",
    "                torch.mean(loss_term_1), torch.mean(loss_term_2), loss_term_3\n",
    "\n",
    "    def eval_at_z(self, z, s, return_beta_hat=False):\n",
    "        # Gives predicted x values at s points when the z value is given\n",
    "        phi_s = self.Phi(s)\n",
    "        beta_hat = self.decoder(z)\n",
    "        x_dec = torch.matmul(phi_s, beta_hat)\n",
    "        if not return_beta_hat:\n",
    "            return x_dec\n",
    "        else:\n",
    "            return x_dec, beta_hat\n",
    "\n",
    "    def draw_samples(self, s, num_samples):\n",
    "        # draw samples from the pi vae\n",
    "        # s should be (num_eval_points, dim)\n",
    "        z_samples = self.normal_sampler.rsample((num_samples, z_dim)).double()\n",
    "        beta_hats = self.decoder(z_samples)\n",
    "        phi_s = self.Phi(s)\n",
    "        x_dec = torch.matmul(beta_hats.unsqueeze(1).unsqueeze(1),\n",
    "            phi_s.unsqueeze(2).unsqueeze(0))\n",
    "        x_dec = x_dec.squeeze()\n",
    "\n",
    "        return x_dec\n",
    "\n",
    "    def get_unnormalized_log_posterior(self, s, x, z):\n",
    "        # Gets something proportional to p(z|x, s) where x and s are new test points\n",
    "        # s (batch x dim)\n",
    "        # x (batch)\n",
    "        # z (z_dim)\n",
    "\n",
    "        log_prior = -0.5 * torch.sum(z**2)\n",
    "\n",
    "        phi_s = self.Phi(s)\n",
    "        beta_hat = self.decoder(z)\n",
    "        x_dec = torch.matmul(phi_s, beta_hat)\n",
    "        log_likelihoods = (-1 / (2 * obs_sigma**2)) * (x_dec - x)**2\n",
    "\n",
    "        return log_prior + torch.sum(log_likelihoods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASS MODEL COMMENTED OUT   \n",
    "class Model(nn.Module):\n",
    "    #_init_ is a reserved method in python, the method _init_ is called when an object is created from a class\n",
    "    #_init_ allows the class to initialise its attributes\n",
    "    \n",
    "    #the word self is used to represent an instance of a class\n",
    "    #By using the keyword self, we can access the attributes and the methods of the class \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       \n",
    "        #initialise the parameters of the Phi radial basis kernel functions: \n",
    "            #the centers are uniformly sampled between xlims and put into a tensor of size num_phi_rbf and input_dim\n",
    "            #and the 2 linear layers composing Phi: Phi takes as input the number of rbfs and outputs betas\n",
    "        \n",
    "        #PHI:    \n",
    "        self.phi_rbf_centers = nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_phi_rbf, input_dim))))\n",
    "        self.phi_nn_1 = nn.Linear(num_phi_rbf, phi_hidden_layer_size)\n",
    "        self.phi_nn_2 = nn.Linear(phi_hidden_layer_size, beta_dim)\n",
    "\n",
    "        \n",
    "        #initialise the parameters of the encoder which takes betas as inputs\n",
    "        #it outputs something of dimensions z_dim*2 (i.e.mu_z and sigma_z, each of dim_z)\n",
    "        #ENCODER:\n",
    "        self.encoder_nn_1 = nn.Linear(beta_dim, encoder_h_dim_1)\n",
    "        self.encoder_nn_2 = nn.Linear(encoder_h_dim_1, encoder_h_dim_2)\n",
    "        self.encoder_nn_3 = nn.Linear(encoder_h_dim_2, encoder_h_dim_3)\n",
    "        self.encoder_nn_4 = nn.Linear(encoder_h_dim_3, z_dim * 2)\n",
    "\n",
    "        #the encoder takes as inputs z and outputs a reconstructed beta\n",
    "        #DECODER:\n",
    "        self.decoder_nn_1 = nn.Linear(z_dim, decoder_h_dim_1)\n",
    "        self.decoder_nn_2 = nn.Linear(decoder_h_dim_1, decoder_h_dim_2)\n",
    "        self.decoder_nn_3 = nn.Linear(decoder_h_dim_2, decoder_h_dim_3)\n",
    "        self.decoder_nn_4 = nn.Linear(decoder_h_dim_3, beta_dim)\n",
    "\n",
    "        #initialise the betas: they are sampled uniformly between -1 and 1. \n",
    "        #number of betas= number of functions trained=num_training_funcs\n",
    "        \n",
    "        #BETAS:\n",
    "        self.betas = nn.Parameter(torch.tensor(np.random.uniform(-1, 1, size=(num_training_funcs, beta_dim))))\n",
    "\n",
    "        #normal N(0,1) sampler\n",
    "        self.normal_sampler = torch.distributions.normal.Normal(0.0, 1.0)\n",
    "        \n",
    "        \n",
    "        #define the high dimensional feature function Phi as an rbf kernel\n",
    "        #it takes the locations s_i^k as inputs\n",
    "        # Takes input (batch x dim_in) and gives Phi(input) (batch x dim_out)\n",
    "        \n",
    "     \n",
    "    def Phi(self, input):\n",
    "        #unsqueeze adds a dimension of size 1 at the specified position\n",
    "        #for the inputs we need to add a dimension along axis=1\n",
    "        #torch.unsqueeze returns a new tensor (here called input_expand)\n",
    "        #with a dimension of size 1 inserted at the specified position 1\n",
    "        #so here input_expand is transformed into a one-column vector\n",
    "        input_expand = torch.unsqueeze(input, 1)\n",
    "        \n",
    "        #same here except that the phi_rbf_centers are transformed into a row vector\n",
    "        #for the rbf centers, an additional dimension needs to be added along x=0\n",
    "        phi_expand = torch.unsqueeze(self.phi_rbf_centers, 0)\n",
    "        \n",
    "        #we now can compute x-x_rbf\n",
    "        M1 = input_expand - phi_expand\n",
    "        \n",
    "        #we now compute sum(x-x')^2 but along axis 2 and I am not sure about what axis 2 represents. \n",
    "        M2 = torch.sum(M1 ** 2, 2)\n",
    "        \n",
    "        #this is exp(-sum(x-x_rbf)^2/sigma_rbf)\n",
    "        M3 = torch.exp(-M2/phi_rbf_sigma)\n",
    "        \n",
    "        #apply a sigmoid function to the first linear layer of Phi\n",
    "        #recall that F stands for torch.nn.functional\n",
    "        M4 = F.sigmoid(self.phi_nn_1(M3))\n",
    "        #apply a second linear layer to sigmoid\n",
    "        M5 = self.phi_nn_2(M4)\n",
    "        return M5\n",
    "\n",
    "    \n",
    "        \n",
    "    def encoder(self, input):\n",
    "    #this function defines the encoder network\n",
    "    # input (batch x beta_dim) output ((batch x z_dim), (batch x z_dim))\n",
    "    #it takes as inputs the betas so the inputs have dimensions (nber of x's, dim of beta)\n",
    "    #the encoder outputs the mean and standard dev of the Normal from which z will be sampled\n",
    "\n",
    "    #apply a relu activation fct to the first linear layer of the encoder\n",
    "        M1 = F.relu(self.encoder_nn_1(input))\n",
    "    #apply another relu to the second linear layer of the encoder. \n",
    "        M2 = F.relu(self.encoder_nn_2(M1))\n",
    "    #apply another relu to the second linear layer of the encoder. \n",
    "        M3 = F.relu(self.encoder_nn_3(M2))\n",
    "    #the fourth layer is simply linear (no activation fct)\n",
    "        M4 = self.encoder_nn_4(M3)\n",
    "\n",
    "    #the first output is the  mean of z: we take the output of the encoder M4\n",
    "    #we retrieve all its rows but only its first z_dim columns\n",
    "        z_mean = M4[:, 0:z_dim]\n",
    "    # The std dev needs to be positive\n",
    "    #for sigma_z, we take all the rows as well bu the subsequent z_dim columns\n",
    "        z_std = torch.exp(M4[:, z_dim:]) \n",
    "        return z_mean, z_std\n",
    "\n",
    "    def decoder(self, input):\n",
    "    # input (batch x z_dim) output (batch x beta_dim)\n",
    "    #the input of the encoder is a sampled z: we have batch x number of them and their dim is z_dim\n",
    "    #we first apply a relu activation to the first linear layer\n",
    "        M1 = F.relu(self.decoder_nn_1(input))\n",
    "    #second and third relu activations on layer 2 and 3\n",
    "        M2 = F.relu(self.decoder_nn_2(M1))\n",
    "        M3 = F.relu(self.decoder_nn_3(M2))\n",
    "    #the output of the decoder \n",
    "    #the last layer is simply linear\n",
    "        M4 = self.decoder_nn_4(M3)\n",
    "        return M4\n",
    "\n",
    "    #REVIEW GET_LOSS FUNCTION\n",
    "\n",
    "    #this function conmputes the loss that we wish to minimise\n",
    "    #its arguments are\n",
    "\n",
    "    def get_loss(self, function_id, s, x, kl_factor, print_breakdown=False, return_breakdown=False):\n",
    "        # function_id is just to know which beta to use\n",
    "        # s are the inputs (batch x dim)\n",
    "        # x are the observed outputs (batch)\n",
    "        batch_size = s.shape[0]\n",
    "        #compute Phi(s)\n",
    "        phi_s = self.Phi(s)\n",
    "        beta = self.betas[function_id, :]\n",
    "        x_enc = torch.matmul(phi_s, beta)\n",
    "\n",
    "        loss_term_1 = (x - x_enc)**2\n",
    "\n",
    "        z_mean, z_std = self.encoder(beta.unsqueeze(0))\n",
    "        # Do we draw one z_sample for all x values of this function or one z_sample for each of them?\n",
    "        # The pi-vae paper in Alg1 does one z-sample for all x-values of the function\n",
    "        z_sample = z_mean + z_std * self.normal_sampler.rsample((1, z_dim))\n",
    "        \n",
    "        beta_hat = self.decoder(z_sample)\n",
    "        \n",
    "        x_dec = torch.matmul(phi_s, beta_hat.squeeze()) # double check this is actually doing what we want it to\n",
    "        loss_term_2 = (x - x_dec)**2\n",
    "\n",
    "    \n",
    "        # You only get one value for beta_hat, not batch_num values, since there's only\n",
    "        # one beta for the whole batch, as the are all from the same function.\n",
    "        # But when you add all the losses together, it will get broadcasted\n",
    "        # so that it is repeated for each item in the batch and the mean will\n",
    "        # be ok\n",
    "        \n",
    "        loss_term_3 = 0.5 * torch.sum(z_std**2 + z_mean**2 - 1 - torch.log(z_std**2),\n",
    "            dim=1)\n",
    "        loss_term_3 = kl_factor * (loss_term_3/z_dim)\n",
    "\n",
    "        if print_breakdown:\n",
    "            # print(\"z_mean, std\", z_mean, z_std)\n",
    "            # print(\"z_samples\", z_samples)\n",
    "            print(\"1\", torch.mean(loss_term_1))\n",
    "            print(\"2\", torch.mean(loss_term_2))\n",
    "            print(\"3\", loss_term_3)\n",
    "\n",
    "        if return_breakdown == False:\n",
    "            return torch.mean(loss_term_1 + loss_term_2) + loss_term_3\n",
    "        else:\n",
    "            return torch.mean(loss_term_1 + loss_term_2) + loss_term_3, \\\n",
    "                torch.mean(loss_term_1), torch.mean(loss_term_2), loss_term_3\n",
    "\n",
    "\n",
    "    # Gives the predicted decoded x values at point locartions s, when the value of z is given\n",
    "    #eval_at_z returns the decoded x's and the decoded betas\n",
    "    def eval_at_z(self, z, s, return_beta_hat=False):\n",
    "        #compute Phi(s_i^k)\n",
    "        phi_s = self.Phi(s)\n",
    "        beta_hat = self.decoder(z)\n",
    "        x_dec = torch.matmul(phi_s, beta_hat)\n",
    "        if not return_beta_hat:\n",
    "            return x_dec\n",
    "        else:\n",
    "            return x_dec, beta_hat\n",
    "\n",
    "\n",
    "    # draw samples from the pi vae: \n",
    "    #we sample z's from N(0,1), pass these z values through the decoder to get beta_hats decoded from z~N(0,1)\n",
    "    #then reconstruct x_hat=beta_hat * Phi(s)\n",
    "    \n",
    "    def draw_samples(self, s, num_samples):\n",
    "\n",
    "        # s should be (num_eval_points, dim)\n",
    "\n",
    "        #dimensions of z_samples: we draw num_samples (nbr of rows) of dim z_dim\n",
    "        #draw z ~ N(0,1)\n",
    "        z_samples = self.normal_sampler.rsample((num_samples, z_dim)).double()\n",
    "        \n",
    "        #decode betas using the z sampled from N(0,1)\n",
    "        beta_hats = self.decoder(z_samples)\n",
    "        #compute Phi(s_i^k)\n",
    "        phi_s = self.Phi(s)\n",
    "        \n",
    "        #compute the decoded x's=beta_hat *phi(s)\n",
    "        #why we do unsqueeze beta_hats and phis_s and then resqueeze x_dec\n",
    "        x_dec = torch.matmul(beta_hats.unsqueeze(1).unsqueeze(1), phi_s.unsqueeze(2).unsqueeze(0))\n",
    "        x_dec = x_dec.squeeze()\n",
    "\n",
    "        return x_dec\n",
    "\n",
    "    # Gets something proportional to p(z|x, s) where x and s are new test points\n",
    "    #p(z|x,s) \\propto p(z)*p(x|z)\n",
    "    # s (batch x dim), x (batch), z (z_dim)\n",
    "    def get_unnormalized_log_posterior(self, s, x, z):\n",
    "\n",
    "        log_prior = -0.5 * torch.sum(z**2)\n",
    "\n",
    "        phi_s = self.Phi(s)\n",
    "        beta_hat = self.decoder(z)\n",
    "        x_dec = torch.matmul(phi_s, beta_hat)\n",
    "        log_likelihoods = (-1 / (2 * obs_sigma**2)) * (x_dec - x)**2\n",
    "\n",
    "        return log_prior + torch.sum(log_likelihoods)\n",
    "    \n",
    "    \n",
    "#Below would be the code if we were drawing one z sample for each of the x's:\n",
    "        # z_samples = z_mean + z_std * self.normal_sampler.rsample((batch_size, z_dim))\n",
    "        # z_samples = z_mean.repeat(batch_size, 1)\n",
    "        # beta_hats = self.decoder(z_samples)\n",
    "        # x_dec = torch.sum(beta_hats * phi_s, dim=1)\n",
    "        # loss_term_2 = (x - x_dec)**2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses Metropolis Hasting algo\n",
    "#An object of the MCMC class returns a sample\n",
    "class MCMC():\n",
    "    def __init__(self, in_model):\n",
    "        self.model = in_model\n",
    "    \n",
    "    def mcmc_draw_samples(self, num_samples, starting_point, proposal_sigma, s_star, x_star):\n",
    "        \n",
    "        z = starting_point\n",
    "        #initialise the samples with a tensor of zeros of dimensions num_samples and z_dim\n",
    "        samples = torch.zeros((num_samples, z_dim)).double()\n",
    "        \n",
    "        #initialise the acceptance probability to 0\n",
    "        acc_prob_sum = 0\n",
    "        \n",
    "        #loop through the number of samples\n",
    "        for t in range(num_samples):\n",
    "            #torch.randn_like(input) returns a tensor of the same size  as the input (i.e dim_z here) \n",
    "            #and the tensor is filled with N(0,1) RVs\n",
    "            #z_p=z + N(0,1)* proposal_sigma^2\n",
    "            #generate a candidate z_p centered at z\n",
    "            z_p = z + torch.randn_like(z) * proposal_sigma**2\n",
    "            \n",
    "            \n",
    "            log_p_z = self.model.get_unnormalized_log_posterior(s_star, x_star, z)\n",
    "            \n",
    "            log_p_z_p = self.model.get_unnormalized_log_posterior(s_star, x_star, z_p)\n",
    "            \n",
    "            #the ratio compares the posterior at the current valuae of z\n",
    "            #and at the new proposed value z_p: it compares p(z|z, s) and p(z_p|x, s)\n",
    "            ratio = torch.exp(log_p_z_p - log_p_z)\n",
    "            \n",
    "            #We take the min(1,p(z_p|x)/p(z|x)):\n",
    "            acc_prob = torch.min(torch.tensor(1.0).double(), ratio)\n",
    "            \n",
    "            #generate u ~ U(0.1)\n",
    "            u = torch.rand(1)\n",
    "            \n",
    "            #tha algo compares the min between 1 and the ratio of the posteriors with a uniform RV u\n",
    "            if u < acc_prob:\n",
    "                z = z_p\n",
    "                \n",
    "            #fill in the tensor with the sampled z    \n",
    "            samples[t, :] = z\n",
    "            \n",
    "            acc_prob_sum += acc_prob.detach().data\n",
    "        print(\"mean acc prob\", acc_prob_sum/num_samples)\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_beta(model, id):\n",
    "    #define some new test points between -5 and 5 at K locations\n",
    "    test_points = torch.arange(-5, 5, 0.1).reshape(100, 1)\n",
    "    \n",
    "    #compute Phi(s^k) at these test points\n",
    "    phi_s = model.Phi(test_points)\n",
    "    \n",
    "    #use the betas method defined in the class model:\n",
    "    beta = model.betas[id, :]\n",
    "    \n",
    "    #compute the encoded x's: x_i,e=beta^T * Phi(s^k)\n",
    "    x_encs = torch.matmul(phi_s, beta)\n",
    "    \n",
    "    #first unsqueeze the betas by adding a dimension to them along the axis 0 and then pass them through the encoder\n",
    "    #the encoder outputs the statistics of N(z; mu_z, std_z)\n",
    "    z_mean, z_std = model.encoder(beta.unsqueeze(0))\n",
    "    print(z_mean, z_std)\n",
    "\n",
    "    #beta_hat is the output of the decoder applied to z_mean\n",
    "    \n",
    "    #why not applied to z_sdv??\n",
    "    \n",
    "    beta_hat = model.decoder(z_mean)\n",
    "    \n",
    "    #the decoded x's are equal to beta_hat^T * Phi(s_k)\n",
    "    x_decs = torch.matmul(beta_hat, torch.transpose(phi_s, 0, 1))\n",
    "    \n",
    "    #plot test points against encoded x's\n",
    "    plt.plot(test_points.detach().numpy(), x_encs.detach().numpy())\n",
    "    \n",
    "    #plot test points against decoded x's\n",
    "    plt.plot(test_points.detach().numpy().reshape(100), x_decs.detach().numpy().reshape(100))\n",
    "    \n",
    "    #plot the original scattered points \n",
    "    plt.scatter(dataset_X[id].reshape(num_eval_points), dataset_f[id].reshape(num_eval_points))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_samples(model, samples, s_star, x_star):\n",
    "    #define some test points between xlims\n",
    "    test_points = torch.arange(function_xlims[0], function_xlims[1], 0.02).double()\n",
    "    \n",
    "    #loop through the number of samples\n",
    "    #one plot for each sample i - so we get samples.shape[0] number of lines\n",
    "    for i in range(samples.shape[0]):\n",
    "        \n",
    "        #method eval_at_z from object of class Model\n",
    "        #the samples[i,:] are the sampled z's\n",
    "        func = model.eval_at_z(samples[i,:], test_points.unsqueeze(1))\n",
    "        \n",
    "        plt.plot(test_points.detach().numpy(), func.detach().numpy(), alpha=0.1, color='black')\n",
    "    \n",
    "    #now we plot the scattered points (x*, s*)\n",
    "    \n",
    "    #EXPLAIN DIFF BETWEEN TESTS POINTS AND STAR POINTS\n",
    "    plt.scatter(s_star.detach().numpy(), x_star.detach().numpy(), s=1000, marker=\"+\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise a dataset of x's and f(x) using the exponential generator\n",
    "# here f(x)=A exp(Bx)\n",
    "\n",
    "dataset_X, dataset_f = generate_maml_sine_dataset()\n",
    "\n",
    "#dataset_X, dataset_f = generate_exp_dataset()\n",
    "#dataset_X, dataset_f = generate_maml_sine_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 1)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 1)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we build the model from the model class\n",
    "#it includes Phi made of 2 linear layers, an encoder composed of 4 layers, a decoder composed of 4 layers\n",
    "model = Model().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (phi_nn_1): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (phi_nn_2): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (encoder_nn_1): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (encoder_nn_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (encoder_nn_3): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (encoder_nn_4): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (decoder_nn_1): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (decoder_nn_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (decoder_nn_3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (decoder_nn_4): Linear(in_features=128, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the class MCMC to your object model\n",
    "mcmc = MCMC(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MCMC at 0x12378dcd0>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the optimiser - using Adam algo and a learning rate of 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the number of input functions f_i\n",
    "num_funcs_to_consider = 1\n",
    "current_max = 32\n",
    "interval = 5\n",
    "nbr_epochs=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now train the model which means we minimise the total of the encoder and decoder using get_loss\n",
    "#define the inputs points as the dataset_X (points between 0 and 1) and the fcts evaluations at x as dataset_f where\n",
    "#y=a*sin(x+phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 1.3820695022009997 l2 0.052775714592218546 l3 0.004431170350392485 num funcs 2\n",
      "l1 0.8925321979329899 l2 0.2034786721123095 l3 0.0027938489521845576 num funcs 2\n",
      "l1 0.33389845610400826 l2 0.13281665106220525 l3 0.0036218327554741426 num funcs 2\n",
      "l1 0.20176670274472674 l2 0.136639346260383 l3 0.0030185765250162807 num funcs 2\n",
      "l1 0.2714020084439108 l2 0.3267842603334868 l3 0.0022774983157294957 num funcs 2\n",
      "l1 0.3617258994889493 l2 0.23468260428602428 l3 0.002376431706665003 num funcs 3\n",
      "l1 2.2173972186136948 l2 0.2965026571054507 l3 0.002381565928200906 num funcs 3\n",
      "l1 1.5484684603421923 l2 0.4408252790309571 l3 0.003071909056255345 num funcs 3\n",
      "l1 0.9040581436470569 l2 0.2529603195230614 l3 0.005478291363399887 num funcs 3\n",
      "l1 0.46956264213391136 l2 0.2587697666461247 l3 0.006997846081514207 num funcs 3\n",
      "l1 0.27253251687084584 l2 0.28283969513164736 l3 0.005895407955848863 num funcs 4\n",
      "l1 0.3316610603212666 l2 0.22526823880437194 l3 0.0034628195328627754 num funcs 4\n",
      "l1 0.3335611902798643 l2 0.20705747953268233 l3 0.002449100722802271 num funcs 4\n",
      "l1 0.3119332915709131 l2 0.2121506727281075 l3 0.0025854388941861347 num funcs 4\n",
      "l1 0.2726880849001712 l2 0.1908518943018399 l3 0.0021687032368689473 num funcs 4\n",
      "l1 0.23617876300158552 l2 0.18853868993464748 l3 0.0019921744097987867 num funcs 5\n",
      "l1 0.27378679007919343 l2 0.14972867438996623 l3 0.0017710578576859153 num funcs 5\n",
      "l1 0.1994654831735343 l2 0.15790540821909604 l3 0.00135563150289107 num funcs 5\n",
      "l1 0.1510231906136655 l2 0.15003993147240174 l3 0.0013857374138614776 num funcs 5\n",
      "l1 0.143822865060248 l2 0.15748158824944944 l3 0.001272496406304959 num funcs 5\n",
      "l1 0.15059301743819312 l2 0.1620376486881234 l3 0.0011515716697407916 num funcs 6\n",
      "l1 0.19763259894506244 l2 0.1536557730095283 l3 0.0008505692122400698 num funcs 6\n",
      "l1 0.15335377968043465 l2 0.16416916624320566 l3 0.0006756405510112543 num funcs 6\n",
      "l1 0.1407581578131531 l2 0.15655150389472103 l3 0.0007377627866113354 num funcs 6\n",
      "l1 0.1451939457756045 l2 0.15115545544950815 l3 0.0006995761092777759 num funcs 6\n",
      "l1 0.14328775835145843 l2 0.15063286153325367 l3 0.000714490054629145 num funcs 7\n",
      "l1 3.99879675407249 l2 0.13491395559933153 l3 0.0005803262683912997 num funcs 7\n",
      "l1 2.102192895293071 l2 0.1400981508097063 l3 0.0003842103959651256 num funcs 7\n",
      "l1 0.8848117881557 l2 0.13860324856999245 l3 0.0003586072571303535 num funcs 7\n",
      "l1 0.3432704332914264 l2 0.13876096167786778 l3 0.0003675511189674794 num funcs 7\n",
      "l1 0.16669910724759643 l2 0.1332647850679968 l3 0.00029284034185138617 num funcs 8\n",
      "l1 0.20035885113023488 l2 0.1445187204343259 l3 0.00024138160058937558 num funcs 8\n",
      "l1 0.16055633972853972 l2 0.14000868743728967 l3 0.00021994598117403922 num funcs 8\n",
      "l1 0.15627383530611455 l2 0.14592518591361303 l3 0.0001926589005084203 num funcs 8\n",
      "l1 0.1517783272446181 l2 0.14103875531444887 l3 0.00030485924526006073 num funcs 8\n",
      "l1 0.14374404021304407 l2 0.14339261432896883 l3 0.0002341945623001362 num funcs 9\n",
      "l1 0.412008300701223 l2 0.12793173503769348 l3 0.00019522045891387767 num funcs 9\n",
      "l1 0.19661519364985056 l2 0.12886776174616366 l3 0.00031748336435753516 num funcs 9\n",
      "l1 0.1285535098896303 l2 0.1297280887884966 l3 0.00033313742574880346 num funcs 9\n",
      "l1 0.13238417435451402 l2 0.129681884076995 l3 0.0002923795552176263 num funcs 9\n",
      "l1 0.13096382579244073 l2 0.12860746943113954 l3 0.0003221165857488796 num funcs 10\n",
      "l1 0.19891754598238748 l2 0.11605802827908136 l3 0.000305597878391674 num funcs 10\n",
      "l1 0.1245140688756305 l2 0.11723337087501544 l3 0.00031536072116102075 num funcs 10\n",
      "l1 0.11739724285358388 l2 0.1192940211465455 l3 0.00023280092004481474 num funcs 10\n",
      "l1 0.117261684529868 l2 0.11849331790362601 l3 0.00046662584378060775 num funcs 10\n",
      "l1 0.11416028988901579 l2 0.12688518935132131 l3 0.0002353991235458398 num funcs 11\n",
      "l1 0.14885153751868083 l2 0.13707912938119546 l3 0.0002132945273948087 num funcs 11\n",
      "l1 0.13196217170944324 l2 0.13583228854475599 l3 0.0001615633914728797 num funcs 11\n",
      "l1 0.13302213479828906 l2 0.1350191646062863 l3 0.0001091712967193613 num funcs 11\n",
      "l1 0.13088407671076066 l2 0.13428456888138915 l3 7.935112661176286e-05 num funcs 11\n",
      "l1 0.13084091225311112 l2 0.13560410286338118 l3 5.7628597679704886e-05 num funcs 12\n",
      "l1 0.25525151259544426 l2 0.15479764973784918 l3 5.588073977652577e-05 num funcs 12\n",
      "l1 0.16459585965438783 l2 0.15511587922003497 l3 5.853950863145451e-05 num funcs 12\n",
      "l1 0.15265676137900025 l2 0.15410052807032873 l3 5.336914671213162e-05 num funcs 12\n",
      "l1 0.1509091769164496 l2 0.15632933418880282 l3 5.92458149504078e-05 num funcs 12\n",
      "l1 0.14974757669618924 l2 0.1534215696043746 l3 5.9052909562269615e-05 num funcs 13\n",
      "l1 0.7278858382403932 l2 0.16213632218760374 l3 4.524696007957781e-05 num funcs 13\n",
      "l1 0.21667258555729466 l2 0.1602192903668921 l3 6.099308044700032e-05 num funcs 13\n",
      "l1 0.1774755342240839 l2 0.1602994795898985 l3 6.458448848746168e-05 num funcs 13\n",
      "l1 0.16456553104023514 l2 0.1654398021865996 l3 8.885927288931197e-05 num funcs 13\n",
      "l1 0.15809672671415226 l2 0.15878803462516242 l3 9.397590333928306e-05 num funcs 14\n",
      "l1 0.1968173262306496 l2 0.17311396232262014 l3 5.707521574599019e-05 num funcs 14\n",
      "l1 0.1613921404408837 l2 0.17213610576047403 l3 0.00013183585709818714 num funcs 14\n",
      "l1 0.16248379065838522 l2 0.17654366935083582 l3 0.00030552948828113706 num funcs 14\n",
      "l1 0.16013525387952554 l2 0.17454053982867285 l3 0.0002612918141339629 num funcs 14\n",
      "l1 0.15993534875497858 l2 0.17834697613020917 l3 0.0002462203205683644 num funcs 15\n",
      "l1 0.16774979973113574 l2 0.1735024151969814 l3 0.0004405969619349902 num funcs 15\n",
      "l1 0.16474843309043274 l2 0.17481329166201456 l3 0.00021384971782267378 num funcs 15\n",
      "l1 0.1602297196124547 l2 0.17287156890858182 l3 0.000149871496555237 num funcs 15\n",
      "l1 0.15997625668031437 l2 0.17016432928059277 l3 0.00010528811311602977 num funcs 15\n",
      "l1 0.15986058114144625 l2 0.17446851534229324 l3 9.501088029208864e-05 num funcs 16\n",
      "l1 0.21142995618639338 l2 0.17968922316111138 l3 7.213718001500429e-05 num funcs 16\n",
      "l1 0.17106592906682955 l2 0.17974897787436667 l3 4.730164206293558e-05 num funcs 16\n",
      "l1 0.1698710775752627 l2 0.1799707850963747 l3 3.3127347170813605e-05 num funcs 16\n",
      "l1 0.168213176160735 l2 0.17798043447889064 l3 3.182174299674952e-05 num funcs 16\n",
      "l1 0.16805363293334316 l2 0.17921979846508518 l3 1.8350655735443967e-05 num funcs 17\n",
      "l1 0.4208999998146526 l2 0.18863354824810113 l3 1.341185517890752e-05 num funcs 17\n",
      "l1 0.20257102979830446 l2 0.18875208592573417 l3 1.3401686790992276e-05 num funcs 17\n",
      "l1 0.18228430155058825 l2 0.18617289332192272 l3 1.751238336742777e-05 num funcs 17\n",
      "l1 0.17750342573005626 l2 0.19069129128332774 l3 1.945268935976463e-05 num funcs 17\n",
      "l1 0.17577402166607478 l2 0.18909683005155642 l3 3.093178366253552e-05 num funcs 18\n",
      "l1 0.17771182197559923 l2 0.19057339449501015 l3 3.0150395947897446e-05 num funcs 18\n",
      "l1 0.18476336521383743 l2 0.18922756824876608 l3 5.9846310344887046e-05 num funcs 18\n",
      "l1 0.18130321706613867 l2 0.18811990816983018 l3 6.342111892338563e-05 num funcs 18\n",
      "l1 0.17745437125395633 l2 0.18722798659513296 l3 2.9573314015745853e-05 num funcs 18\n",
      "l1 0.17668280745212528 l2 0.1875745986446512 l3 2.046623970401218e-05 num funcs 19\n",
      "l1 0.27061968143006154 l2 0.1827371928381204 l3 4.5884787744475387e-05 num funcs 19\n",
      "l1 0.183909797303938 l2 0.18317484709615445 l3 3.885693344388404e-05 num funcs 19\n",
      "l1 0.17399379372618268 l2 0.18478152734462444 l3 2.2259889641071038e-05 num funcs 19\n",
      "l1 0.1726770474999107 l2 0.18353275790988482 l3 2.6647862961267343e-05 num funcs 19\n",
      "l1 0.17209775598226063 l2 0.18364206876221253 l3 1.159477132030468e-05 num funcs 20\n",
      "l1 0.4519843350446807 l2 0.17716697392997371 l3 8.934254098545334e-06 num funcs 20\n",
      "l1 0.19504697277743682 l2 0.1785700850495709 l3 8.58842315243949e-06 num funcs 20\n",
      "l1 0.1776966237624136 l2 0.17736808081029093 l3 8.192919269236516e-06 num funcs 20\n",
      "l1 0.16802564985259846 l2 0.17713904153530005 l3 7.83318911424707e-06 num funcs 20\n",
      "l1 0.16732518574746424 l2 0.17645048163785756 l3 5.736643686639414e-06 num funcs 21\n",
      "l1 0.2611115662554757 l2 0.1848810400160738 l3 4.6642051422015205e-06 num funcs 21\n",
      "l1 0.18672014664731337 l2 0.18971176050943744 l3 2.3906210101477053e-05 num funcs 21\n",
      "l1 0.17897087751611518 l2 0.19121412601432605 l3 3.76543388611529e-05 num funcs 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.17588688087131205 l2 0.1882596028289704 l3 6.008690834976606e-05 num funcs 21\n",
      "l1 0.17535313640676572 l2 0.1863650780301302 l3 2.261247457823038e-05 num funcs 22\n",
      "l1 0.27788531082511864 l2 0.1930180685462929 l3 1.0358353018787832e-05 num funcs 22\n",
      "l1 0.2001308882518509 l2 0.19382929141563485 l3 7.084932241674933e-06 num funcs 22\n",
      "l1 0.19099862278105692 l2 0.19426643009679437 l3 1.6768020124839745e-05 num funcs 22\n",
      "l1 0.18577189452312634 l2 0.19423412856732708 l3 1.688241375970684e-05 num funcs 22\n",
      "l1 0.1828499108196705 l2 0.1930955322121595 l3 1.8884367065848204e-05 num funcs 23\n",
      "l1 0.18048251000779883 l2 0.18849834226303158 l3 1.2056768254236965e-05 num funcs 23\n",
      "l1 0.18211680018924445 l2 0.18854734529019335 l3 2.7165607812194074e-05 num funcs 23\n",
      "l1 0.17832163493784736 l2 0.18747635829333142 l3 3.970710682017221e-05 num funcs 23\n",
      "l1 0.17628403786080768 l2 0.18770481125488966 l3 1.5943760845069788e-05 num funcs 23\n",
      "l1 0.17549347616192265 l2 0.1886361509660579 l3 1.2040586275892078e-05 num funcs 24\n",
      "l1 0.1835392324074757 l2 0.18248070194695476 l3 1.6422354425373385e-05 num funcs 24\n",
      "l1 0.17812329208581093 l2 0.183459034764437 l3 1.1682966449222438e-05 num funcs 24\n",
      "l1 0.17398902957368204 l2 0.1821651348390226 l3 1.9834619541982502e-05 num funcs 24\n",
      "l1 0.1742381837923037 l2 0.18246836345925524 l3 1.4136459880111446e-05 num funcs 24\n",
      "l1 0.17448689598119652 l2 0.18317857988891398 l3 2.013496458975617e-05 num funcs 25\n",
      "l1 0.22985914419366738 l2 0.17579418541639227 l3 3.0205634957063395e-05 num funcs 25\n",
      "l1 0.1913565423069987 l2 0.1763531526401073 l3 1.8675246203835714e-05 num funcs 25\n",
      "l1 0.19806189382152542 l2 0.1774501745963253 l3 3.206537570664035e-05 num funcs 25\n",
      "l1 0.19062239985837656 l2 0.1756967733583784 l3 1.6518473856323846e-05 num funcs 25\n",
      "l1 0.1873890631222678 l2 0.1776498165672473 l3 2.059558035326504e-05 num funcs 26\n",
      "l1 0.1860504939164926 l2 0.18728926330436818 l3 9.68613759880135e-06 num funcs 26\n",
      "l1 0.18793812350987513 l2 0.18816536493744132 l3 6.184368741698978e-06 num funcs 26\n",
      "l1 0.18664922244390697 l2 0.1869187936945434 l3 5.927524146075929e-06 num funcs 26\n",
      "l1 0.18132172283587747 l2 0.18736403878652164 l3 6.977009777168433e-06 num funcs 26\n",
      "l1 0.1780979613023124 l2 0.1873395314981529 l3 9.234115144154763e-06 num funcs 27\n",
      "l1 0.27169134887591684 l2 0.18271128358178323 l3 1.2170638948302798e-05 num funcs 27\n",
      "l1 0.18839599867795634 l2 0.18363562973262185 l3 1.632078345819196e-05 num funcs 27\n",
      "l1 0.17530835786254292 l2 0.183919712967053 l3 1.564496908794338e-05 num funcs 27\n",
      "l1 0.17199797200775266 l2 0.18325818033472277 l3 7.334601750879514e-05 num funcs 27\n",
      "l1 0.1682477799939047 l2 0.1845659936255847 l3 6.225122228137836e-05 num funcs 28\n",
      "l1 0.3150250225092653 l2 0.17605271344678883 l3 3.3934685773331714e-05 num funcs 28\n",
      "l1 0.1876620965899074 l2 0.18011380769016688 l3 7.128411579563116e-05 num funcs 28\n",
      "l1 0.1665633612363698 l2 0.17548343569751584 l3 4.7106415469354517e-05 num funcs 28\n",
      "l1 0.15831952786207698 l2 0.1750089634242562 l3 2.285812704363904e-05 num funcs 28\n",
      "l1 0.15666022590173306 l2 0.17498784108974916 l3 6.911351102435256e-06 num funcs 29\n",
      "l1 0.19213628488986287 l2 0.17240407471051492 l3 5.9566172039867e-06 num funcs 29\n",
      "l1 0.1690960045553849 l2 0.17313844365131897 l3 8.348202740637072e-06 num funcs 29\n",
      "l1 0.16238716747421852 l2 0.1732538296553666 l3 1.6315030753222874e-05 num funcs 29\n",
      "l1 0.15985086555560002 l2 0.17160959031204415 l3 9.49750553800318e-06 num funcs 29\n",
      "l1 0.1576303418119872 l2 0.17223883383647232 l3 4.594821531519458e-06 num funcs 30\n",
      "l1 0.20861170617731312 l2 0.17331375101382684 l3 3.5826822206862973e-06 num funcs 30\n",
      "l1 0.16811675311014937 l2 0.17369979910549055 l3 4.897485627742059e-06 num funcs 30\n",
      "l1 0.1615017916643247 l2 0.1735020848992526 l3 4.686680822920507e-06 num funcs 30\n",
      "l1 0.15886200597221725 l2 0.1733171223198409 l3 2.050571885279423e-06 num funcs 30\n",
      "l1 0.1572489874474604 l2 0.17323040245480428 l3 2.7936511971062035e-06 num funcs 31\n",
      "l1 0.16030677593798134 l2 0.1706684407687451 l3 9.840569478431355e-06 num funcs 31\n",
      "l1 0.16064264220882243 l2 0.16986801156927606 l3 8.774938914745427e-06 num funcs 31\n",
      "l1 0.1578583404280106 l2 0.17030286165286584 l3 3.7943950159803156e-06 num funcs 31\n",
      "l1 0.15673972542513775 l2 0.1706431762349862 l3 3.1863148798346232e-06 num funcs 31\n",
      "l1 0.15558775427858682 l2 0.17017362952442314 l3 1.5294822814533585e-05 num funcs 32\n",
      "l1 0.1899572565377851 l2 0.16542252806013652 l3 2.597810387027329e-06 num funcs 32\n",
      "l1 0.16358267002896 l2 0.16555937609929172 l3 1.3143848320482365e-06 num funcs 32\n",
      "l1 0.1654989676319755 l2 0.16524575500125782 l3 1.2397228758546667e-06 num funcs 32\n",
      "l1 0.16392807885989086 l2 0.16536975052653216 l3 3.1170849587570967e-06 num funcs 32\n",
      "l1 0.16475980510657062 l2 0.16547602586265978 l3 9.310176124346246e-06 num funcs 32\n",
      "l1 0.16329664036403863 l2 0.16445388084074689 l3 1.2409683223751238e-05 num funcs 32\n",
      "l1 0.1618200856647481 l2 0.16621023397143755 l3 2.7721787081546248e-05 num funcs 32\n",
      "l1 0.160371210629958 l2 0.163839546228855 l3 2.4368269260535066e-05 num funcs 32\n",
      "l1 0.15971679253779725 l2 0.16848071220130545 l3 3.414535953413e-05 num funcs 32\n",
      "l1 0.15918684637108219 l2 0.16510103225708583 l3 7.815553304788687e-06 num funcs 32\n",
      "l1 0.15940959512677613 l2 0.16551855905877605 l3 3.040894397193186e-06 num funcs 32\n",
      "l1 0.15913276487827 l2 0.16497429272496494 l3 1.3062870486463002e-06 num funcs 32\n",
      "l1 0.15949730050434707 l2 0.16545753051236706 l3 1.2406924620607812e-06 num funcs 32\n",
      "l1 0.15916979001835183 l2 0.16494134562122992 l3 1.124646843898372e-06 num funcs 32\n",
      "l1 0.1593259634456448 l2 0.1656476536111205 l3 1.3395231912788947e-06 num funcs 32\n",
      "l1 0.15897591038546746 l2 0.16495518567729828 l3 8.380427780150965e-07 num funcs 32\n",
      "l1 0.15900500647589538 l2 0.16543348862086707 l3 4.829947677977409e-07 num funcs 32\n",
      "l1 0.1586292320029351 l2 0.16488996554607155 l3 3.410322763457597e-07 num funcs 32\n",
      "l1 0.15857076736907946 l2 0.1655158839978669 l3 7.213781259552296e-07 num funcs 32\n",
      "l1 0.1581080891296671 l2 0.16492250655299062 l3 5.249058540932531e-07 num funcs 32\n",
      "l1 0.15795366196773158 l2 0.16536356228410268 l3 4.041744548871973e-07 num funcs 32\n",
      "l1 0.15743731865574734 l2 0.16492213965595148 l3 4.1032156145586464e-07 num funcs 32\n",
      "l1 0.1571397569739305 l2 0.16548397461460368 l3 1.0989532252890602e-06 num funcs 32\n",
      "l1 0.15659872747488968 l2 0.16501956610528443 l3 4.6577598218606065e-07 num funcs 32\n",
      "l1 0.15622980955736465 l2 0.16554913069610755 l3 9.198525935514228e-07 num funcs 32\n",
      "l1 0.15577035768879988 l2 0.1649245790188013 l3 2.750850828013699e-07 num funcs 32\n",
      "l1 0.1554604146655612 l2 0.1653830622330017 l3 4.3744408702446314e-07 num funcs 32\n",
      "l1 0.15513941649761312 l2 0.16491385192200933 l3 2.1113622220041333e-06 num funcs 32\n",
      "l1 0.15493168979647043 l2 0.16544155967647445 l3 8.636644067107424e-07 num funcs 32\n",
      "l1 0.15472841468223145 l2 0.16507097919765254 l3 5.652278592206579e-06 num funcs 32\n",
      "l1 0.15458368211136286 l2 0.16524389519157312 l3 9.871469778943369e-07 num funcs 32\n",
      "l1 0.15440136041688768 l2 0.1652074244198049 l3 1.0539893354784418e-06 num funcs 32\n",
      "l1 0.15426263960861392 l2 0.165352094836123 l3 1.4550587264332042e-06 num funcs 32\n",
      "l1 0.15410632979877442 l2 0.16493077156616848 l3 4.950142543407236e-07 num funcs 32\n",
      "l1 0.15396270641840748 l2 0.16529571107572077 l3 4.73952891376685e-07 num funcs 32\n",
      "l1 0.15384390670729964 l2 0.16499035394825684 l3 3.050708686664304e-07 num funcs 32\n",
      "l1 0.1536961173700574 l2 0.16522707551710436 l3 4.50047728948326e-07 num funcs 32\n",
      "l1 0.15356934973264286 l2 0.16501968859123856 l3 2.808838092785133e-07 num funcs 32\n",
      "l1 0.1533853900963005 l2 0.16516492540885708 l3 2.0192480014862704e-07 num funcs 32\n",
      "l1 0.1532007035301422 l2 0.16508431727333156 l3 2.5777430381551385e-07 num funcs 32\n",
      "l1 0.15295937526343056 l2 0.16506641667432626 l3 2.2899137038186668e-07 num funcs 32\n",
      "l1 0.15269170827896078 l2 0.16517718928384015 l3 1.3140740926337503e-07 num funcs 32\n",
      "l1 0.15239831970611933 l2 0.16499210404604456 l3 1.7476479047517037e-07 num funcs 32\n",
      "l1 0.15207445782456858 l2 0.16525652114773465 l3 1.9660027231713526e-07 num funcs 32\n",
      "l1 0.15180090919402528 l2 0.16492311731853604 l3 1.2865691515161254e-07 num funcs 32\n",
      "l1 0.15153427250369625 l2 0.1653235124829439 l3 3.1009035829387233e-07 num funcs 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.15147195366189042 l2 0.16484422116220337 l3 3.333914480196134e-07 num funcs 32\n",
      "l1 0.15155649282898243 l2 0.16539427879898339 l3 2.796513051229967e-07 num funcs 32\n",
      "l1 0.15221328846329896 l2 0.16483445936483224 l3 2.3819825409686202e-07 num funcs 32\n",
      "l1 0.1534540621902379 l2 0.1653640265277412 l3 4.2320607430179534e-07 num funcs 32\n",
      "l1 0.15636230197682416 l2 0.16496312029719956 l3 5.488500525305935e-07 num funcs 32\n",
      "l1 0.16152291375301897 l2 0.16507487068273277 l3 4.7136123357144234e-07 num funcs 32\n",
      "l1 0.16989295847000824 l2 0.16537533290490003 l3 6.638152331162704e-07 num funcs 32\n",
      "l1 0.18047973895150002 l2 0.1647217804802022 l3 1.1935917626142794e-06 num funcs 32\n",
      "l1 0.1861865697871236 l2 0.16572994674174496 l3 1.2040790126514996e-06 num funcs 32\n",
      "l1 0.18623017263198854 l2 0.16463247741633058 l3 1.2164967909306784e-06 num funcs 32\n",
      "l1 0.17929158408216073 l2 0.1655927205052228 l3 2.423652951427952e-06 num funcs 32\n",
      "l1 0.169036097217095 l2 0.16463726005709967 l3 2.6340550602650044e-06 num funcs 32\n",
      "l1 0.16163531163274658 l2 0.16538022588491205 l3 2.3752963675453516e-06 num funcs 32\n",
      "l1 0.15616076110791474 l2 0.16575864134759064 l3 8.338941813250232e-06 num funcs 32\n",
      "l1 0.1531359079761545 l2 0.16592894948107356 l3 3.33484273013986e-05 num funcs 32\n",
      "l1 0.15138753345155914 l2 0.16454227891718445 l3 2.9149226834423876e-06 num funcs 32\n",
      "l1 0.1504843291493554 l2 0.16480377815293057 l3 1.869329533429675e-05 num funcs 32\n",
      "l1 0.1500815577349548 l2 0.1673601869169838 l3 6.0646850205901465e-05 num funcs 32\n",
      "l1 0.14939515081214633 l2 0.16484745183242322 l3 2.3986507931906696e-05 num funcs 32\n",
      "l1 0.14897876669082338 l2 0.16572850698624197 l3 1.0643003894394138e-05 num funcs 32\n",
      "l1 0.1486719837178167 l2 0.16553226220216727 l3 3.6452783601819384e-06 num funcs 32\n",
      "l1 0.1485073354652744 l2 0.16485994315018895 l3 2.5747838573903154e-06 num funcs 32\n",
      "l1 0.1483804501908389 l2 0.16494788985202807 l3 5.794831038944687e-07 num funcs 32\n",
      "l1 0.1483449514561342 l2 0.16514656223478985 l3 1.440253474771466e-06 num funcs 32\n",
      "l1 0.1483413857835596 l2 0.16500850790288804 l3 6.058430192971981e-06 num funcs 32\n",
      "l1 0.14843698632362198 l2 0.16498703358648117 l3 1.1885701049627421e-05 num funcs 32\n",
      "l1 0.1486583003693443 l2 0.16535527383364984 l3 1.901648123223041e-06 num funcs 32\n",
      "l1 0.14886892438458904 l2 0.16485710721446087 l3 3.545064683116202e-06 num funcs 32\n",
      "l1 0.14903580741300632 l2 0.16509778436745126 l3 2.203990857484788e-06 num funcs 32\n",
      "l1 0.14927284817003295 l2 0.1657430824484456 l3 4.696447043373897e-06 num funcs 32\n",
      "l1 0.14954205447269683 l2 0.16520130776811282 l3 3.923932942017085e-06 num funcs 32\n",
      "l1 0.15008102788312816 l2 0.16519333519674198 l3 2.3932562819001264e-06 num funcs 32\n",
      "l1 0.15102161262114516 l2 0.16477305016693872 l3 6.107244790265048e-07 num funcs 32\n",
      "l1 0.1522814730242083 l2 0.1650781500946562 l3 1.0965639115558169e-07 num funcs 32\n",
      "l1 0.1549450569888339 l2 0.16487992660716885 l3 6.96319538104052e-08 num funcs 32\n",
      "l1 0.158791360551162 l2 0.16468054118304282 l3 5.3611655551005556e-08 num funcs 32\n",
      "l1 0.16515698187256422 l2 0.16551030676618878 l3 4.355964960885489e-08 num funcs 32\n",
      "l1 0.1728191198137206 l2 0.16428868553672418 l3 3.6285996713049114e-08 num funcs 32\n",
      "l1 0.17305575481711866 l2 0.1658214477970777 l3 3.083342217909599e-08 num funcs 32\n",
      "l1 0.1709377210505956 l2 0.16440932062393232 l3 2.66908682796297e-08 num funcs 32\n",
      "l1 0.16566240975348975 l2 0.16533041399021356 l3 3.399147750607893e-08 num funcs 32\n",
      "l1 0.1624159420343223 l2 0.16459484228548255 l3 2.8435712181868533e-08 num funcs 32\n",
      "l1 0.15902289710249196 l2 0.165108929325014 l3 3.4692699210755666e-08 num funcs 32\n",
      "l1 0.15603131311162946 l2 0.16463572646715935 l3 3.5455505134714225e-08 num funcs 32\n",
      "l1 0.154052174340213 l2 0.16506964554909928 l3 5.389872885910162e-08 num funcs 32\n",
      "l1 0.15250761678429223 l2 0.16466331128123468 l3 8.212166290118476e-07 num funcs 32\n",
      "l1 0.1514393751206115 l2 0.16505786601604927 l3 6.859875784864323e-07 num funcs 32\n",
      "l1 0.15067160404000682 l2 0.16475678594578316 l3 3.369985733965191e-07 num funcs 32\n",
      "l1 0.15007401817672356 l2 0.16504486092353599 l3 2.2132212744731128e-07 num funcs 32\n",
      "l1 0.14968613084113408 l2 0.16461542117235167 l3 5.165854547320461e-08 num funcs 32\n",
      "l1 0.14933975060667978 l2 0.16501597432230908 l3 3.220124980428874e-08 num funcs 32\n",
      "l1 0.1491298672157247 l2 0.16469771561562674 l3 1.5885479741984803e-07 num funcs 32\n",
      "l1 0.1489301188818461 l2 0.16496285498488283 l3 9.681435288989757e-08 num funcs 32\n",
      "l1 0.1488082930418227 l2 0.16468393885596982 l3 7.586175554343767e-08 num funcs 32\n",
      "l1 0.1486904935608128 l2 0.16490714792922084 l3 2.641355479183128e-08 num funcs 32\n",
      "l1 0.1486288590858964 l2 0.16470992172087245 l3 2.1395710616246137e-08 num funcs 32\n",
      "l1 0.14858789973033498 l2 0.16489435242741496 l3 3.529184196743165e-08 num funcs 32\n",
      "l1 0.14861558950003345 l2 0.16470409570350697 l3 1.7034698878846264e-08 num funcs 32\n",
      "l1 0.148701175333724 l2 0.16491761305026909 l3 1.558007258307961e-08 num funcs 32\n",
      "l1 0.14889575922663645 l2 0.16465823322932022 l3 4.094316787843553e-08 num funcs 32\n",
      "l1 0.14920683596652556 l2 0.1649027056620272 l3 1.0646539142424348e-07 num funcs 32\n",
      "l1 0.14978382819306213 l2 0.16479121076621772 l3 1.2880692137810412e-05 num funcs 32\n",
      "l1 0.1505487434413939 l2 0.16512423606618187 l3 2.0216280304361352e-06 num funcs 32\n",
      "l1 0.15146438347083097 l2 0.1645130743174889 l3 2.462256927585385e-07 num funcs 32\n",
      "l1 0.1524171538854714 l2 0.16516561893852688 l3 5.769391099077712e-07 num funcs 32\n",
      "l1 0.15354712849940313 l2 0.16457166832238596 l3 2.1313579200503532e-07 num funcs 32\n",
      "l1 0.15490422642862856 l2 0.16514078947922795 l3 3.3392176525739506e-07 num funcs 32\n",
      "l1 0.15632083079126438 l2 0.16454820315910124 l3 1.0135036987248082e-06 num funcs 32\n",
      "l1 0.1583477618745323 l2 0.1651342805503453 l3 1.6244354859862097e-07 num funcs 32\n",
      "l1 0.1605900887714458 l2 0.16444477785336808 l3 5.226676666705866e-07 num funcs 32\n",
      "l1 0.16342335658225357 l2 0.16539899092496518 l3 1.337447344978792e-07 num funcs 32\n",
      "l1 0.16571228101906793 l2 0.16419596051296886 l3 2.30149544543967e-07 num funcs 32\n",
      "l1 0.1651696223101755 l2 0.16560721819442703 l3 1.5280189556999777e-06 num funcs 32\n",
      "l1 0.1632578661449621 l2 0.1643419771775136 l3 5.118774435721577e-07 num funcs 32\n",
      "l1 0.15865647406479566 l2 0.16529912223266796 l3 1.0982209790406803e-06 num funcs 32\n",
      "l1 0.1552827577478188 l2 0.16446546083950003 l3 6.90844348096203e-08 num funcs 32\n",
      "l1 0.15255922738011654 l2 0.16498288686230028 l3 2.140175187914644e-08 num funcs 32\n",
      "l1 0.15101203275868527 l2 0.1645916747062093 l3 1.8886703849521201e-07 num funcs 32\n",
      "l1 0.15013923490324163 l2 0.1648649940084396 l3 1.4980515962824768e-07 num funcs 32\n",
      "l1 0.14966680140924024 l2 0.16463433136422095 l3 1.1394411482979774e-07 num funcs 32\n",
      "l1 0.14938611423868195 l2 0.16483873622884165 l3 2.3289057117406197e-06 num funcs 32\n",
      "l1 0.1492216803830182 l2 0.16463697557783635 l3 3.7732828679903644e-07 num funcs 32\n",
      "l1 0.14919343293343734 l2 0.16480457347702368 l3 4.427228809580628e-08 num funcs 32\n",
      "l1 0.14936052580095077 l2 0.16464318702485892 l3 2.594220438687426e-08 num funcs 32\n",
      "l1 0.14976700297223086 l2 0.16478038291470287 l3 2.080033886418656e-08 num funcs 32\n",
      "l1 0.15053834592565718 l2 0.16468030327323846 l3 1.716782219271012e-08 num funcs 32\n",
      "l1 0.15169617829081378 l2 0.16471355492683115 l3 1.473520073752438e-08 num funcs 32\n",
      "l1 0.15335856067772913 l2 0.16476792550864636 l3 1.2859674689040996e-08 num funcs 32\n",
      "l1 0.15531151658946732 l2 0.16459951351972227 l3 1.1396118355847673e-08 num funcs 32\n",
      "l1 0.15736294552373745 l2 0.16488092148511796 l3 1.0091861679496096e-08 num funcs 32\n",
      "l1 0.1589426003258974 l2 0.1644999878641869 l3 9.075461152942812e-09 num funcs 32\n",
      "l1 0.1595852246948922 l2 0.16494715014450356 l3 8.080052443508668e-09 num funcs 32\n",
      "l1 0.15907025916984913 l2 0.16446956339567353 l3 7.339019977000061e-09 num funcs 32\n",
      "l1 0.1573754872399906 l2 0.16493303066684836 l3 6.60042415486739e-09 num funcs 32\n",
      "l1 0.15537136817610997 l2 0.16449165922747627 l3 6.0573102836479865e-09 num funcs 32\n",
      "l1 0.1533749797600709 l2 0.16487864549774994 l3 6.043328717904454e-09 num funcs 32\n",
      "l1 0.15189005216695317 l2 0.16452339680245076 l3 5.318714492995861e-09 num funcs 32\n",
      "l1 0.15082126946631702 l2 0.16482981310338607 l3 4.783264452207983e-09 num funcs 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.15014674410239015 l2 0.16454309761825844 l3 4.643400461677749e-09 num funcs 32\n",
      "l1 0.1497277390480467 l2 0.16480275560865365 l3 6.062671191330679e-09 num funcs 32\n",
      "l1 0.14949603930957078 l2 0.16454777457411607 l3 4.19918147021905e-09 num funcs 32\n",
      "l1 0.14940394714883792 l2 0.16479038482171243 l3 4.077667339482123e-09 num funcs 32\n",
      "l1 0.14945121683472778 l2 0.16454910269321726 l3 3.9688938554674545e-09 num funcs 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train for nbr_epochs\n",
    "for epoch_id in range(nbr_epochs):\n",
    "    # print(epoch_id)\n",
    "    \n",
    "    #create empty arrays for the losses l1, l2 and l3:\n",
    "    l1s = []\n",
    "    l2s = []\n",
    "    l3s = []\n",
    "    \n",
    "    for function_id in range(num_funcs_to_consider):\n",
    "        #because pytorch accunulates the gradients on successive backward passes, \n",
    "        #we need to re-initialise them to 0 at every loop\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #the inputs are made of the dataset\n",
    "        #what is dataset_X?\n",
    "        input_points = torch.tensor(dataset_X[function_id])\n",
    "        \n",
    "        #the x's are the functions evaluations\n",
    "        #what is dataset_f?\n",
    "        x_vals = torch.tensor(dataset_f[function_id])\n",
    "        \n",
    "        #when breakdown is true, the get_loss method returns: \n",
    "        #torch.mean(loss_term_1 + loss_term_2) + loss_term_3, torch.mean(loss_term_1), torch.mean(loss_term_2), loss_term_3\n",
    "        #so here: \n",
    "            #loss=mean(loss_term_1 + loss_term_2) + loss_term_3\n",
    "            #l1=mean(loss_term_1)\n",
    "            #l2=torch.mean(loss_term_2)\n",
    "            #l3=torch.mean(loss_term_3)\n",
    "        \n",
    "        loss, l1, l2, l3 = model.get_loss(function_id, input_points, x_vals, 1.0, return_breakdown=True)\n",
    "        \n",
    "        #loss_backward() computes the gradients for every parameter and accumulates them \n",
    "        loss.backward()\n",
    "        #optimizer.step() updates the value of x using the gradient of x computed by loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        l1s.append(l1.detach().numpy())\n",
    "        l2s.append(l2.detach().numpy())\n",
    "        l3s.append(l3.detach().numpy())\n",
    "\n",
    "    if epoch_id % interval == 0:\n",
    "        num_funcs_to_consider = min(num_funcs_to_consider+1, current_max)\n",
    "\n",
    "    print(\"l1\", np.mean(np.array(l1s)),\n",
    "        \"l2\", np.mean(np.array(l2s)),\n",
    "        \"l3\", np.mean(np.array(l3s)),\n",
    "        \"num funcs\", num_funcs_to_consider)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw some samples from the pivae:\n",
    "#First define locations between function_xlims\n",
    "locations = torch.arange(function_xlims[0], function_xlims[1], 0.2).unsqueeze(1).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('locations_output_sine_201120.txt', locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw samples using the method draw_samples() from the class model:\n",
    "#here we draw z from z ~ N(0,1), pass the z values through the decoder to get beta_hat \n",
    "#and reconstruct x_hat=beta_hat*Phi(s_i^k)\n",
    "\n",
    "#from the locations s_i^k, we draw samples, and obtain samples of decoded x's\n",
    "samples = model.draw_samples(locations, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('samples_output_sine_201120.txt', samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc prob tensor(0.9887, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# ---- MCMC ----\n",
    "#define the additional pairs of points (s,x) at which we will compute the log posterior log p(x^*,z) \n",
    "#to get the Metropolis Hasting algo to sample the z_p that satisfy the criterium of acceptance\n",
    "\n",
    "s_star = torch.tensor([0,2,-2]).unsqueeze(1).double()\n",
    "x_star = torch.tensor([0.5*np.sin(-1.5),0.5*np.sin(2-1.5),0.5*np.sin(-2-1.5)]).double()\n",
    "\n",
    "\n",
    "#draw z from N(0,1)\n",
    "z = torch.randn((z_dim,)).double()\n",
    "\n",
    "#draw 10,000 samples from the mcmc algo\n",
    "mcmc_samples = mcmc.mcmc_draw_samples(10000, z, 0.1, s_star, x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 16])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples = mcmc_samples[1000::100,:]\n",
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean acc prob tensor(0.9841, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9854, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9832, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9844, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9832, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9833, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9837, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9822, dtype=torch.float64)\n",
      "mean acc prob tensor(0.9807, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#apply MCMC to different starting points z ~ N(0,1)\n",
    "for i in range(9):\n",
    "    z = torch.randn((z_dim,)).double()\n",
    "    mcmc_samples = mcmc.mcmc_draw_samples(1000, z, 0.1, s_star, x_star)\n",
    "    \n",
    "    all_samples[10*i:10*(i+1), :] = mcmc_samples[500::50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 16])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('all_samples_output_sine_3pts.txt', all_samples)\n",
    "np.savetxt('s_star_output_sine_3pts.txt', s_star)\n",
    "np.savetxt('x_star_output_sine_3pts.txt', x_star)\n",
    "np.savetxt('mcmc_samples_output_sine_3pts.txt', mcmc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test points between -5 and 5. \n",
    "test_points = torch.arange(-5, 5, 0.1).unsqueeze(1)\n",
    "np.save('test_points', test_points)\n",
    "np.savetxt('test_points_out.txt', test_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the functions evaluations:\n",
    "\n",
    "#first create an np array of zeros to store the fct evaluations: \n",
    "    #nbr of rows = nbr of sample fcts\n",
    "    #nbr of columns = nbr of test points\n",
    "all_funcs=np.zeros((all_samples.shape[0], test_points.shape[0]))\n",
    "\n",
    "#eval_at_z computes: beta_hat^T * Phi(s) \n",
    "#where beta_hat=decoder(z) and here z=sample from mcmc\n",
    "for i in range(all_samples.shape[0]):\n",
    "        func = model.eval_at_z(all_samples[i,:], test_points)\n",
    "        all_funcs[i,:]=func.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('all_funcs_out_3pts.txt', all_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(self, function_id, s, x, kl_factor, print_breakdown=False, return_breakdown=False):\n",
    "         # function_id is just to know which beta to use\n",
    "        # s are the inputs (batch x dim)\n",
    "        # x are the observed outputs (batch)\n",
    "        \n",
    "            #the first dim of s is the number N of training functions (i=1, ..., N)\n",
    "            #here: N=x batch I think?\n",
    "            batch_size = s.shape[0]\n",
    "        \n",
    "            #we directly apply the feature function Phi to our location points s, this is Phi(s_i^k)\n",
    "            phi_s = self.Phi(s)\n",
    "        \n",
    "            #beta is builds as an array. Not sure where betas method is defined\n",
    "            beta = self.betas[function_id, :]\n",
    "        \n",
    "            #this corresponds to the equation: x_hat_i^k,e=beta_i^T * Phi(s_i^k)\n",
    "            x_enc = torch.matmul(phi_s, beta)\n",
    "\n",
    "            #the first loss term is the squared difference between the inputs x and their encoded form hat_x's\n",
    "            loss_term_1 = (x - x_enc)**2\n",
    "\n",
    "            #to get the output mu_z and sigma_z of the encoder, we apply the encoder to the betas. \n",
    "            #this corresponds to the  equation: [z_mu, z_sd]^T=e(eta_e, beta_i)\n",
    "            # we add a second dimention to the betas - we unsqueeze them along axis 0 \n",
    "        \n",
    "            z_mean, z_std = self.encoder(beta.unsqueeze(0))\n",
    "        \n",
    "           # Do we draw one z_sample for all x values of this function or one z_sample for each of them?\n",
    "        \n",
    "            # The pi-vae paper in Alg1 does one z-sample for all x-values of the function\n",
    "            #z ~ N(mu_z, sigma_z) -> z=mu_z+sigma_z * N(0,1)\n",
    "        \n",
    "            #here we draw ONLY ONE z_sample for all x_values of this particular function\n",
    "            z_sample = z_mean + z_std * self.normal_sampler.rsample((1, z_dim))\n",
    "        \n",
    "            #then we apply the decoder to the sampled z in order to retrieve the reconstructed betas\n",
    "            beta_hat = self.decoder(z_sample)\n",
    "            \n",
    "            #the reconstructed x's: x_hat_i^k,d=beta_hat_i^T * Phi(s_i^k)\n",
    "            # double check this is actually doing what we want it to\n",
    "            x_dec = torch.matmul(phi_s, beta_hat.squeeze()) # double check this is actually doing what we want it to\n",
    "        \n",
    "            #the second loss term compares the intial x's with the reconstructed x's\n",
    "            loss_term_2 = (x - x_dec)**2\n",
    "\n",
    "            # z_samples = z_mean + z_std * self.normal_sampler.rsample((batch_size, z_dim))\n",
    "            # z_samples = z_mean.repeat(batch_size, 1)\n",
    "            # beta_hats = self.decoder(z_samples)\n",
    "            # x_dec = torch.sum(beta_hats * phi_s, dim=1)\n",
    "            # loss_term_2 = (x - x_dec)**2\n",
    "            # beta_hat = self.decoder(z_mean)\n",
    "            # beta_hat = beta_hat.reshape(beta.shape)\n",
    "            # loss_term_2 = torch.mean((beta_hat - beta)**2)\n",
    "\n",
    "\n",
    "\n",
    "            # You only get one value not batch_num values since there's only\n",
    "            # one beta for the whole batch since they're all from the same function\n",
    "            # But when you add all the losses together, it will get broadcasted\n",
    "            # so that it is repeated for each item in the batch so the mean will\n",
    "            # be ok\n",
    "            loss_term_3 = 0.5 * torch.sum(z_std**2 + z_mean**2 - 1 - torch.log(z_std**2),\n",
    "                dim=1)\n",
    "            loss_term_3 = kl_factor * (loss_term_3/z_dim)\n",
    "\n",
    "            if print_breakdown:\n",
    "                # print(\"z_mean, std\", z_mean, z_std)\n",
    "                # print(\"z_samples\", z_samples)\n",
    "                print(\"1\", torch.mean(loss_term_1))\n",
    "                print(\"2\", torch.mean(loss_term_2))\n",
    "                print(\"3\", loss_term_3)\n",
    "\n",
    "            if return_breakdown == False:\n",
    "                return torch.mean(loss_term_1 + loss_term_2) + loss_term_3\n",
    "            else:\n",
    "                return torch.mean(loss_term_1 + loss_term_2) + loss_term_3, \\\n",
    "                    torch.mean(loss_term_1), torch.mean(loss_term_2), loss_term_3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
