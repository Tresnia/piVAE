{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters initialisation:\n",
    "\n",
    "#dimension of the vector beta_i: it has p features we wanrt to learn. Here p=beta_dim\n",
    "beta_dim = 100\n",
    "\n",
    "#The input x is a scalar and here its dimension is 1\n",
    "input_dim = 1\n",
    "\n",
    "#we need to set up a few parameters for the high dimensional feature function phi\n",
    "#here phi is assumed to be a radial basis kernel function\n",
    "#number of rbf centers\n",
    "num_phi_rbf = 100\n",
    "#sigma of the radial basis function\n",
    "phi_rbf_sigma = 5\n",
    "#we need to learn phi using a NN. We assume it has 2 hidden layers, each with 10 neurons\n",
    "phi_hidden_layer_size = 10\n",
    "\n",
    "\n",
    "#dimension of the latent variable z that embeds the inputs:\n",
    "z_dim = 16\n",
    "\n",
    "# we train our VAE with 1000 training functions:\n",
    "#this also gives the numbers of betas to learn:\n",
    "num_training_funcs = 1000\n",
    "\n",
    "\n",
    "#each function f_i is evaluated at K locations. Here K=num_eval_points\n",
    "num_eval_points = 20\n",
    "\n",
    "#The observation standard deviation\n",
    "obs_sigma = 0.01 \n",
    "\n",
    "#the encoder and the decoder parts each have 3 hidden layers\n",
    "encoder_h_dim_1 = 512\n",
    "encoder_h_dim_2 = 512\n",
    "encoder_h_dim_3 = 128\n",
    "\n",
    "decoder_h_dim_1 = 128\n",
    "decoder_h_dim_2 = 128\n",
    "decoder_h_dim_3 = 128\n",
    "\n",
    "#f_i=x_i is the function evaluation at the K locations: lower and upper bounds of x\n",
    "function_xlims = [-5, 5]\n",
    "function_slims=[-10,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo 1: Prior training for piVAE\n",
    "## 1. Draw N functions evaluated at K points: \n",
    "### For $i=1, \\ldots, N,$ the N training functions are called $f_i$, the K locations are $s_i^k,$ and the function evaluations are $x_i^k=f_i(s_i^k).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_fcts_dataset():\n",
    "    s_locations=[]\n",
    "    x_fcts_evals=[]\n",
    "    \n",
    "    #loop through the number of training functions N=num_training_funcs\n",
    "    #each sin wave is differentiated by a uniformly sampled amplitude and phase\n",
    "    \n",
    "    for i in range(num_training_funcs):\n",
    "        s=np.random.uniform(function_slims[0], function_slims[1], size=(num_eval_points, 1))\n",
    "        amplitude=np.random.uniform(1.0,5.0)\n",
    "        phase = np.random.uniform(0, np.pi)\n",
    "        x=amplitude*np.sin(s+phase)\n",
    "        s_locations.append(s)\n",
    "        x_fcts_evals.append(x)\n",
    "    return np.array(s_locations), np.array(x_fcts_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s, dataset_x=generate_sine_fcts_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For each training function $i=1,\\ldots, N$ and for each location $k=1, \\ldots, K$, we compute  $\\hat{x}_{e,i}^k=\\beta_i^T\\Phi(s_i^k).$ \n",
    "\n",
    "But before we can do so, we need to create a class Model  that will encapsulate the feature function $\\Phi$ and also the encoder, the decoder and the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.phi_rbf_centers = nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_phi_rbf, input_dim))))\n",
    "        self.phi_nn_1 = nn.Linear(num_phi_rbf, phi_hidden_layer_size)\n",
    "        self.phi_nn_2 = nn.Linear(phi_hidden_layer_size, beta_dim)\n",
    "          \n",
    "        self.encoder_nn_1=nn.Linear(beta_dim, encoder_h_dim_1)\n",
    "        self.encoder_nn_2=nn.Linear(encoder_h_dim_1, encoder_h_dim_2)\n",
    "        self.encoder_nn_3=nn.Linear(encoder_h_dim_2, encoder_h_dim_3)\n",
    "        self.encoder_nn_4=nn.Linear(encoder_h_dim_3, z_dim*2) #the encoder output is z_mean and z_sigma \n",
    "         \n",
    "        #we instantiate the hidden layers of the decoder neural network\n",
    "        #the input of the decoder is z and once z is passed through the decoder, it outputs a reconstructed beta\n",
    "        self.decoder_nn_1=nn.Linear(z_dim, decoder_h_dim_1)\n",
    "        self.decoder_nn_2=nn.Linear(decoder_h_dim_1, decoder_h_dim_2)\n",
    "        self.decoder_nn_3=nn.Linear(decoder_h_dim_2, decoder_h_dim_3)\n",
    "        self.decoder_nn_4=nn.Linear(decoder_h_dim_3, beta_dim)\n",
    "        \n",
    "\n",
    "        # self.betas = nn.Parameter(torch.ones(num_training_funcs, beta_dim))\n",
    "        self.betas = nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(-1, 1, size=(num_training_funcs, beta_dim))\n",
    "        ))\n",
    "\n",
    "        #instantiate the N(0,1) distribution\n",
    "        self.normal_sampler = torch.distributions.normal.Normal(0.0, 1.0)\n",
    "    \n",
    "    \n",
    "      \n",
    "    #the model contains the feature function Phi\n",
    "    #Phi takes as input the locations s_i^k, which need to be tensors\n",
    "    def Phi(self, input):\n",
    "        # Takes input (batch x dim_in) and gives Phi(input) (batch x dim_out)\n",
    "        input_expand = torch.unsqueeze(input, 1)\n",
    "        phi_expand = torch.unsqueeze(self.phi_rbf_centers, 0)\n",
    "        M1 = input_expand - phi_expand\n",
    "        M2 = torch.sum(M1 ** 2, 2)\n",
    "        M3 = torch.exp(-M2/phi_rbf_sigma)\n",
    "        M4 = F.sigmoid(self.phi_nn_1(M3))\n",
    "        M5 = self.phi_nn_2(M4)\n",
    "        return M5\n",
    "    \n",
    "    \n",
    "    #encoder inputs beta and outputs z_mean and z_std:\n",
    "    def encoder(self, input):\n",
    "        #The input of the encoder is beta. Each beta has dimension beta_dim \n",
    "        #but we need one beta for each function evaluation, i.e. batch_size\n",
    "        #thus the inputs dimensions are input (batch x beta_dim)\n",
    "        #The outputs of the decoder are the mean and standard dev of the db of z. \n",
    "        #Dimensions of output: ((batch x z_dim), (batch x z_dim))\n",
    "        \n",
    "        #we apply relu to the linear hidden layers:\n",
    "        M1=F.relu(self.encoder_nn_1(input))\n",
    "        M2=F.relu(self.encoder_nn_2(M1))\n",
    "        M3 = F.relu(self.encoder_nn_3(M2))\n",
    "                  \n",
    "        #the last layer of the encoder is simply linear          \n",
    "        M4 = self.encoder_nn_4(M3) \n",
    "                  \n",
    "        z_mean=M4[:, 0:z_dim]\n",
    "        #we take the exponential of the weights to ensure the std is positive\n",
    "        z_std=torch.exp(M4[:,z_dim:])\n",
    "                  \n",
    "        return z_mean, z_std  \n",
    "    \n",
    "    \n",
    "    #decoder inputs the latent variable z and outputs a recontructed beta\n",
    "    def decoder(self, input):\n",
    "        #input dimensions: (batch x z_dim) \n",
    "        #output dimensions: (batch x beta_dim)\n",
    "        M1=F.relu(self.decoder_nn_1(input))\n",
    "        M2=F.relu(self.decoder_nn_2(M1))\n",
    "        M3=F.relu(self.decoder_nn_3(M2))\n",
    "        M4=self.decoder_nn_4(M3)\n",
    "        return M4\n",
    "    \n",
    "    \n",
    "    \n",
    "    #the loss is made of 3 terms: \n",
    "    def get_loss(self, function_id, s, x, kl_factor, print_breakdown=False, return_breakdown=False):\n",
    "        #the arguments of the loss functions are the function_id (to differentiate which fct eval is becing considered)\n",
    "        #we also need the locations s_i^k and the function evaluations x_i^k\n",
    "        #the batch_size is equal to the first dimension of the tensor s\n",
    "        batch_size=s.shape[0] \n",
    "        \n",
    "        #FIRST LOSS TERM:\n",
    "        phi_s=self.Phi(s)\n",
    "        beta=self.betas[function_id, :]\n",
    "        x_enc=torch.matmul(phi_s,beta)\n",
    "        \n",
    "        loss_term_1=(x_enc-x)**2\n",
    "        \n",
    "        \n",
    "        #SECOND LOSS TERM:\n",
    "        #pass the beta through the encoder to get the mean and std of the latent db of z\n",
    "        z_mean, z_std=self.encoder(beta.unsqueeze(0))\n",
    "        \n",
    "        #sample z: we draw one z for each function_id\n",
    "        z_sample = z_mean + z_std * self.normal_sampler.rsample((1, z_dim))        \n",
    "        \n",
    "        #we then decode the z_sample to obtain the reconstructed beta\n",
    "        hat_beta=self.decoder(z_sample)\n",
    "        \n",
    "        #we need to compute the reconstructed x from the hat_beta\n",
    "        x_dec=torch.matmul(phi_s,hat_beta.squeeze())\n",
    "        \n",
    "        loss_term_2=(x_dec-x)**2\n",
    "        \n",
    "        #regularisation term between N(z_mean, z_std) and N(0,1):\n",
    "        \n",
    "        loss_term_3=0.5*torch.sum(z_mean**2+z_std**2-1-torch.log(z_std**2), dim=1)\n",
    "    \n",
    "        #DOES THE KL-FACTOR COME FROM THE FACT THAT WE NEED TO LEARN THE BETAS?\n",
    "        #WHY DO WE DIVIDE BY Z_DIM?\n",
    "        loss_term_3 = kl_factor * (loss_term_3/z_dim)\n",
    "        \n",
    "        \n",
    "        if print_breakdown:\n",
    "            #if print_breakdown is true, we print the mean of each of the first 2 loss terms: \n",
    "            #(mean over the training funcs)\n",
    "            #loss_term_1 and loss_term_2 both depend on beta and hat_beta and thus on the training function considered\n",
    "            #loss_term_3 is only one term (does not depend on beta_i)\n",
    "            print(\"1\", torch.mean(loss_term_1))\n",
    "            print(\"2\", torch.mean(loss_term_2))\n",
    "            print(\"3\", loss_term_3)\n",
    "            \n",
    "            \n",
    "        if return_breakdown == False:\n",
    "            return torch.mean(loss_term_1 + loss_term_2) + loss_term_3\n",
    "        else:\n",
    "            return torch.mean(loss_term_1 + loss_term_2) + loss_term_3, \\\n",
    "                torch.mean(loss_term_1), torch.mean(loss_term_2), loss_term_3\n",
    "        \n",
    "    \n",
    "    #this function computes the predicted x at the location s when the value of z is given\n",
    "    def predicted_x(self, s, z, return_hat_beta=False):\n",
    "        phi_s=self.Phi(s)\n",
    "        hat_beta=self.decoder(z)\n",
    "        x_dec=torch.matmul(phi_s, hat_beta)\n",
    "        if not return_hat_beta:\n",
    "            return x_dec\n",
    "        else:\n",
    "            return x_dec, hat_beta\n",
    "        \n",
    "    #this function draws a standard normal latent z from N(0,1), passes z through the decoder and reconstructs \n",
    "    #the functions valuations x's\n",
    "    \n",
    "    def draw_sample_from_piVAE(self, s, num_samples):\n",
    "        phi_s=self.Phi(s)\n",
    "        #draw z from the standard normal N(0,1)\n",
    "        z_samples=self.normal_sampler.rsample((num_samples, z_dim)).double()\n",
    "        hat_betas=self.decoder(z_samples)\n",
    "        x_dec=torch.matmul(hat_betas.unsqueeze(1).unsqueeze(1), phi_s.unsqueeze(2).unsqueeze(0))\n",
    "        x_dec=x_dec.squeeze()\n",
    "        \n",
    "        return x_dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 1 is the MSE between the observed function valuations $x_i$'s and the $x_{d,i}^k=\\beta_i^T\\Phi(s_i^k):$\n",
    "\n",
    "### loss1 $=\\frac{1}{2}\\sum_i \\sum_k (\\beta_i^T\\Phi(s_i^k)-x_i)^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 2 is the MSE between the observed function valuations $x_i$'s and the  decoded  $x_{d,i}^k=\\hat\\beta_i^T\\Phi(s_i^k):$\n",
    "\n",
    "### loss2 $=\\frac{1}{2}\\sum_i \\sum_k (\\hat\\beta_i^T\\Phi(s_i^k)-x_i)^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 3 is the regularisation term. It is the KL divergence between N(z; z_mean, z_std) and N(z; 0,1). See appendic B of Kingma and Welling: \n",
    "### Regularisation term between a Gaussian and N(0,1): $-D_{KL}=\\frac{1}{2}\\sum_j (1+\\log \\sigma_j^2-\\mu_j^2-\\sigma_j^2).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a model\n",
    "\n",
    "model=Model().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (phi_nn_1): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (phi_nn_2): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (encoder_nn_1): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (encoder_nn_2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (encoder_nn_3): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (encoder_nn_4): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (decoder_nn_1): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (decoder_nn_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (decoder_nn_3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (decoder_nn_4): Linear(in_features=128, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr=1e-3\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model: we optimise the loss using back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "#what is the difference between the number_funcs_to_consider and number of training functions (N in the paper)\n",
    "#this gives us the number of functions we consider at each epoh\n",
    "num_funcs_to_consider=500\n",
    "#maybe the functions to consider is simply a subset of the training functions\n",
    "\n",
    "interval=2\n",
    "current_max=300\n",
    "kl_factor=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we have 5 training functions f_i which we evaluate at K=20 locations\n",
    "dataset_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the 20 locations for the first training function\n",
    "#dataset_s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 0\n",
      "loss_term_1: 6.441614452799898 loss_term_2: 5.0318177541430895 loss_term_3: 0.0013364252973675711 number fcts: 300\n",
      "epoch number: 1\n",
      "loss_term_1: 5.248699688167946 loss_term_2: 5.045365981293769 loss_term_3: 0.00046946131670622126 number fcts: 300\n",
      "epoch number: 2\n",
      "loss_term_1: 5.065456590735872 loss_term_2: 5.044956092495338 loss_term_3: 0.00030560516593664824 number fcts: 300\n",
      "epoch number: 3\n",
      "loss_term_1: 4.981373301192847 loss_term_2: 5.036549702634773 loss_term_3: 0.0010370670381505053 number fcts: 300\n",
      "epoch number: 4\n",
      "loss_term_1: 4.922282770278792 loss_term_2: 5.034013153239072 loss_term_3: 0.0029106047145529518 number fcts: 300\n",
      "epoch number: 5\n",
      "loss_term_1: 4.872865830564001 loss_term_2: 5.005597086579388 loss_term_3: 0.004775487188987786 number fcts: 300\n",
      "epoch number: 6\n",
      "loss_term_1: 4.828866316538846 loss_term_2: 4.957449632951987 loss_term_3: 0.008394692355714918 number fcts: 300\n",
      "epoch number: 7\n",
      "loss_term_1: 4.795759406223443 loss_term_2: 4.9102018491432435 loss_term_3: 0.009867890223271728 number fcts: 300\n",
      "epoch number: 8\n",
      "loss_term_1: 4.774355837110993 loss_term_2: 4.818957764166258 loss_term_3: 0.009980827549169236 number fcts: 300\n",
      "epoch number: 9\n",
      "loss_term_1: 4.763116114563432 loss_term_2: 4.803127441204216 loss_term_3: 0.010505797593686673 number fcts: 300\n"
     ]
    }
   ],
   "source": [
    "for epoch_id in range(num_epochs):\n",
    "    print(\"epoch number:\", epoch_id)\n",
    "\n",
    "    \n",
    "    #create empty storages for the 3 loss terms\n",
    "    loss_1=[]\n",
    "    loss_2=[]\n",
    "    loss_3=[]\n",
    "    \n",
    "    \n",
    "    for fct_id in range(num_funcs_to_consider):\n",
    "        #ensure the gradients are back to 0 after each loop for a new function\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #define the inputs s and x\n",
    "        \n",
    "        #extract the locations s for the function that is being considered \n",
    "        s=dataset_s[fct_id]\n",
    "        #transform into a tensor\n",
    "        s=torch.tensor(s)\n",
    "        \n",
    "        #extract the function valuations for the function that is being considered and transform into a tensor\n",
    "        x=dataset_x[fct_id]\n",
    "        x=torch.tensor(x)\n",
    "        \n",
    "        loss, l1, l2, l3=model.get_loss(fct_id, s, x, kl_factor, return_breakdown=True)\n",
    "        \n",
    "        #compute the backward gradients of loss which is equal to \n",
    "        #loss=mean(l1+l2)+l3\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_1.append(l1.detach())\n",
    "        loss_2.append(l2.detach())\n",
    "        loss_3.append(l3.detach())\n",
    "        \n",
    "        \n",
    "        \n",
    "    if epoch_id % interval == 0:\n",
    "        num_funcs_to_consider = min(num_funcs_to_consider+1, current_max)\n",
    "        \n",
    "        \n",
    "    print('loss_term_1:', np.mean(np.array(loss_1)),\n",
    "             'loss_term_2:',  np.mean(np.array(loss_2)),\n",
    "             'loss_term_3:', np.mean(np.array(loss_3)), \n",
    "             'number fcts:', num_funcs_to_consider)\n",
    "             \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Draw some samples from the pivae\n",
    "num_samples=5\n",
    "locations = torch.arange(-10, 10, 0.2).unsqueeze(1).double()\n",
    "locations = locations.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the locations in a txt file\n",
    "np.savetxt('locations.txt', locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "piVAE_samples=model.draw_sample_from_piVAE(locations, num_samples)\n",
    "\n",
    "piVAE_samples = piVAE_samples.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the piVAE samples in a text file\n",
    "np.savetxt('piVAE_samples.txt', piVAE_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION: HOW DO WE KNOW WHEN TO UNSQUEEZE. DO WE NEED TENSORS OF SAME DIMENSIONS?\n",
    "#get Phi to work outside the class as a stand-alone function\n",
    "# Takes input (batch x dim_in) and gives Phi(input) (batch x dim_out)\n",
    "\n",
    "phi_rbf_centers=nn.Parameter(torch.tensor(\n",
    "            np.random.uniform(function_xlims[0], function_xlims[1],\n",
    "            size=(num_phi_rbf,input_dim) )))\n",
    "phi_nn_1=nn.Linear(num_phi_rbf, phi_hidden_layer_size)\n",
    "phi_nn_2=nn.Linear(phi_hidden_layer_size, beta_dim)\n",
    "\n",
    "\n",
    "def Phi(input):\n",
    "        \n",
    "        input_expand = torch.unsqueeze(input, 1)\n",
    "        phi_expand = torch.unsqueeze(phi_rbf_centers, 0)\n",
    "        M1 = input_expand - phi_expand\n",
    "        M2 = torch.sum(M1 ** 2, 2)\n",
    "        M3 = torch.exp(-M2/phi_rbf_sigma)\n",
    "        M4 = F.sigmoid(phi_nn_1(M3))\n",
    "        M5 = phi_nn_2(M4)\n",
    "        return M5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
